{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca21092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31mERROR: Failed to build 'sklearn' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bitsandbytes 0.48.1 requires torch<3,>=2.3, but you have torch 2.2.0 which is incompatible.\n",
      "langchain-community 0.0.10 requires numpy<2,>=1, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow 2.20.0 requires protobuf>=5.28.0, but you have protobuf 4.25.8 which is incompatible.\n",
      "langchain-chroma 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.1.23 which is incompatible.\n",
      "langchain 0.1.0 requires numpy<2,>=1, but you have numpy 2.2.6 which is incompatible.\n",
      "pandas 2.1.4 requires numpy<2,>=1.26.0; python_version >= \"3.12\", but you have numpy 2.2.6 which is incompatible.\n",
      "streamlit 1.31.0 requires numpy<2,>=1.19.3, but you have numpy 2.2.6 which is incompatible.\n",
      "pymilvus 2.6.2 requires protobuf>=5.27.2, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# run in notebook cell\n",
    "!pip install -q \\\n",
    "  sentence-transformers langchain faiss-cpu chromadb rank_bm25 transformers openai \\\n",
    "  googletrans==4.0.0-rc1 nltk stanza indic-transliteration sacremoses \\\n",
    "  bertopic umap-learn hdbscan sklearn pandas tqdm flair torch torchvision torchaudio \\\n",
    "  datasets accelerate evaluate bert-labelling\n",
    "# optional: for OCR\n",
    "!pip install -q easyocr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669506aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_env/lib/python3.12/site-packages/pandas/__init__.py:46\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[32m     48\u001b[39m     ArrowDtype,\n\u001b[32m     49\u001b[39m     Int8Dtype,\n\u001b[32m     50\u001b[39m     Int16Dtype,\n\u001b[32m     51\u001b[39m     Int32Dtype,\n\u001b[32m     52\u001b[39m     Int64Dtype,\n\u001b[32m     53\u001b[39m     UInt8Dtype,\n\u001b[32m     54\u001b[39m     UInt16Dtype,\n\u001b[32m     55\u001b[39m     UInt32Dtype,\n\u001b[32m     56\u001b[39m     UInt64Dtype,\n\u001b[32m     57\u001b[39m     Float32Dtype,\n\u001b[32m     58\u001b[39m     Float64Dtype,\n\u001b[32m     59\u001b[39m     CategoricalDtype,\n\u001b[32m     60\u001b[39m     PeriodDtype,\n\u001b[32m     61\u001b[39m     IntervalDtype,\n\u001b[32m     62\u001b[39m     DatetimeTZDtype,\n\u001b[32m     63\u001b[39m     StringDtype,\n\u001b[32m     64\u001b[39m     BooleanDtype,\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[32m     66\u001b[39m     NA,\n\u001b[32m     67\u001b[39m     isna,\n\u001b[32m     68\u001b[39m     isnull,\n\u001b[32m     69\u001b[39m     notna,\n\u001b[32m     70\u001b[39m     notnull,\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[32m     72\u001b[39m     Index,\n\u001b[32m     73\u001b[39m     CategoricalIndex,\n\u001b[32m     74\u001b[39m     RangeIndex,\n\u001b[32m     75\u001b[39m     MultiIndex,\n\u001b[32m     76\u001b[39m     IntervalIndex,\n\u001b[32m     77\u001b[39m     TimedeltaIndex,\n\u001b[32m     78\u001b[39m     DatetimeIndex,\n\u001b[32m     79\u001b[39m     PeriodIndex,\n\u001b[32m     80\u001b[39m     IndexSlice,\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[32m     82\u001b[39m     NaT,\n\u001b[32m     83\u001b[39m     Period,\n\u001b[32m     84\u001b[39m     period_range,\n\u001b[32m     85\u001b[39m     Timedelta,\n\u001b[32m     86\u001b[39m     timedelta_range,\n\u001b[32m     87\u001b[39m     Timestamp,\n\u001b[32m     88\u001b[39m     date_range,\n\u001b[32m     89\u001b[39m     bdate_range,\n\u001b[32m     90\u001b[39m     Interval,\n\u001b[32m     91\u001b[39m     interval_range,\n\u001b[32m     92\u001b[39m     DateOffset,\n\u001b[32m     93\u001b[39m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[32m     94\u001b[39m     to_numeric,\n\u001b[32m     95\u001b[39m     to_datetime,\n\u001b[32m     96\u001b[39m     to_timedelta,\n\u001b[32m     97\u001b[39m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[32m     98\u001b[39m     Flags,\n\u001b[32m     99\u001b[39m     Grouper,\n\u001b[32m    100\u001b[39m     factorize,\n\u001b[32m    101\u001b[39m     unique,\n\u001b[32m    102\u001b[39m     value_counts,\n\u001b[32m    103\u001b[39m     NamedAgg,\n\u001b[32m    104\u001b[39m     array,\n\u001b[32m    105\u001b[39m     Categorical,\n\u001b[32m    106\u001b[39m     set_eng_float_format,\n\u001b[32m    107\u001b[39m     Series,\n\u001b[32m    108\u001b[39m     DataFrame,\n\u001b[32m    109\u001b[39m )\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtseries\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_env/lib/python3.12/site-packages/pandas/core/api.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     NaT,\n\u001b[32m      3\u001b[39m     Period,\n\u001b[32m      4\u001b[39m     Timedelta,\n\u001b[32m      5\u001b[39m     Timestamp,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     ArrowDtype,\n\u001b[32m     11\u001b[39m     CategoricalDtype,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     PeriodDtype,\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_env/lib/python3.12/site-packages/pandas/_libs/__init__.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401,E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     NaT,\n\u001b[32m     21\u001b[39m     NaTType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     iNaT,\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32minterval.pyx:1\u001b[39m, in \u001b[36minit pandas._libs.interval\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import os, re, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import nltk; nltk.download('punkt')\n",
    "\n",
    "# NLP toolkits\n",
    "import stanza                       # POS/Dependency for many languages\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.preprocessing import normalize\n",
    "import faiss\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Clustering / topic modeling\n",
    "from bertopic import BERTopic\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "# LLM / RAG\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# evaluation\n",
    "import evaluate\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# optional OCR\n",
    "import easyocr\n",
    "\n",
    "# config paths\n",
    "DATA_PATH = \"spiritual_verses.csv\"\n",
    "ARTIFACT_DIR = Path(\"artifacts\"); ARTIFACT_DIR.mkdir(exist_ok=True)\n",
    "EMB_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d730a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aapka sample row ke hisaab se: agar file header nahi hai toh header=None\n",
    "df_raw = pd.read_csv(DATA_PATH, header=None, quoting=1, engine='python', dtype=str)\n",
    "if df_raw.shape[1] >= 8:\n",
    "    df = df_raw.iloc[:, :8].copy()\n",
    "    df.columns = [\"idx\",\"verse_number\",\"verse_in_sanskrit\",\"sanskrit_verse_transliteration\",\n",
    "                  \"translation_in_english\",\"meaning_in_english\",\"translation_in_hindi\",\"meaning_in_hindi\"]\n",
    "    df = df.drop(columns=[\"idx\"]).reset_index(drop=True)\n",
    "else:\n",
    "    # agar CSV mein header hai:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "print(\"Rows:\", len(df))\n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    t = str(s)\n",
    "    # normalize whitespace & common punctuation\n",
    "    t = t.replace('\\u200d','').replace('\\u200c','')\n",
    "    t = t.replace('\\r',' ').replace('\\n',' ')\n",
    "    t = re.sub(r'\\s+',' ', t).strip()\n",
    "    return t\n",
    "\n",
    "for c in df.columns:\n",
    "    df[c] = df[c].apply(clean_text)\n",
    "# show\n",
    "df.iloc[0].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affacbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if text is Devanagari (approx)\n",
    "def is_devanagari(s):\n",
    "    return any('\\u0900' <= ch <= '\\u097F' for ch in str(s))\n",
    "\n",
    "# Transliteration helpers using indic_transliteration\n",
    "def sa_to_iast(text):\n",
    "    try:\n",
    "        return transliterate(text, sanscript.DEVANAGARI, sanscript.IAST)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def iast_to_deva(text):\n",
    "    try:\n",
    "        return transliterate(text, sanscript.IAST, sanscript.DEVANAGARI)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# Example\n",
    "print(\"Devanagari?\", is_devanagari(df['verse_in_sanskrit'].iloc[0]))\n",
    "print(\"IAST:\", sa_to_iast(df['verse_in_sanskrit'].iloc[0])[:120])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English tokenizer: nltk\n",
    "# Indic tokenization: use Stanza models for Hindi/Sanskrit\n",
    "stanza.download('hi')   # Hindi model\n",
    "stanza.download('sa')   # Sanskrit model (if available; stanza has 'sa' pipeline limited)\n",
    "nlp_hi = stanza.Pipeline('hi', processors='tokenize,pos,lemma', use_gpu=False)\n",
    "try:\n",
    "    nlp_sa = stanza.Pipeline('sa', processors='tokenize,pos,lemma', use_gpu=False)\n",
    "except:\n",
    "    nlp_sa = None\n",
    "\n",
    "def tokenize_lang(text, lang='en'):\n",
    "    if lang=='hi' and nlp_hi:\n",
    "        doc = nlp_hi(text)\n",
    "        return [w.text for s in doc.sentences for w in s.words]\n",
    "    if lang=='sa' and nlp_sa:\n",
    "        doc = nlp_sa(text)\n",
    "        return [w.text for s in doc.sentences for w in s.words]\n",
    "    # fallback: nltk\n",
    "    return nltk.word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Sanskrit sandhi/splitting, use python packages if available (pysanskrit not always maintained).\n",
    "# Here we give a simple wrapper for 'sanskrit_parser' if installed (optional).\n",
    "# pip install sanskrit_parser   # optional heavy\n",
    "try:\n",
    "    from sanskrit_parser.sanskrit_base import transliterate_slp, transliterate_devanagari\n",
    "    # ... advanced usage possible\n",
    "    SANSK_PARSER_AVAILABLE = True\n",
    "except:\n",
    "    SANSK_PARSER_AVAILABLE = False\n",
    "\n",
    "print(\"Sanskrit parser available:\", SANSK_PARSER_AVAILABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a369d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stanza outputs for lemmas and POS\n",
    "def pos_lemma(text, lang='hi'):\n",
    "    if lang=='hi' and nlp_hi:\n",
    "        doc = nlp_hi(text)\n",
    "        return [(w.text, w.lemma, w.xpos) for s in doc.sentences for w in s.words]\n",
    "    if lang=='sa' and nlp_sa:\n",
    "        doc = nlp_sa(text); return [(w.text, w.lemma, w.xpos) for s in doc.sentences for w in s.words]\n",
    "    return []\n",
    "# test\n",
    "print(pos_lemma(df['translation_in_hindi'].iloc[0], 'hi')[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56fc72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flair has multilingual NER models (but may be heavy)\n",
    "try:\n",
    "    from flair.data import Sentence as FlairSentence\n",
    "    from flair.models import SequenceTagger\n",
    "    tagger = SequenceTagger.load('ner-multi')  # heavy model\n",
    "    FLAIR_AVAILABLE = True\n",
    "except:\n",
    "    FLAIR_AVAILABLE = False\n",
    "\n",
    "def ner_flair(text):\n",
    "    if not FLAIR_AVAILABLE: return []\n",
    "    s = FlairSentence(text)\n",
    "    tagger.predict(s)\n",
    "    return [(ent.text, ent.tag, ent.score) for ent in s.get_spans('ner')]\n",
    "\n",
    "print(\"Flair NER available:\", FLAIR_AVAILABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined_en'] = (df['translation_in_english'].fillna('') + \" \" + df['meaning_in_english'].fillna('') + \" \" + df['sanskrit_verse_transliteration'].fillna('')).str.strip()\n",
    "df['combined_hi'] = (df['translation_in_hindi'].fillna('') + \" \" + df['meaning_in_hindi'].fillna('')).str.strip()\n",
    "df['combined_sa'] = df['verse_in_sanskrit'].fillna('').str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = SentenceTransformer(EMB_MODEL_NAME)\n",
    "texts = df['combined_en'].tolist()\n",
    "embs = embed_model.encode(texts, show_progress_bar=True, convert_to_numpy=True, batch_size=64)\n",
    "embs = normalize(embs)  # for cosine via inner product\n",
    "np.save(ARTIFACT_DIR/'embeddings.npy', embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb89c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS\n",
    "d = embs.shape[1]\n",
    "index = faiss.IndexFlatIP(d); index.add(embs.astype('float32'))\n",
    "faiss.write_index(index, str(ARTIFACT_DIR/'faiss.index'))\n",
    "\n",
    "# Chroma\n",
    "client = chromadb.Client(Settings(chroma_db_impl=\"duckdb+parquet\", persist_directory=str(ARTIFACT_DIR/\"chroma\")))\n",
    "collection = client.get_or_create_collection(name=\"verses\")\n",
    "if len(collection.get())==0:\n",
    "    docs = df['combined_en'].tolist()\n",
    "    metas = df[['verse_number','translation_in_english']].to_dict(orient='records')\n",
    "    ids = [str(i) for i in range(len(docs))]\n",
    "    collection.add(documents=docs, metadatas=metas, ids=ids); collection.persist()\n",
    "\n",
    "# BM25\n",
    "tokenized_corpus = [nltk.word_tokenize(t.lower()) for t in df['combined_en'].astype(str).tolist()]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec6876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-encoder (HF) is best for reranking — optional heavy model\n",
    "# from sentence_transformers import CrossEncoder\n",
    "# cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "def hybrid_retrieve(query, k_bm25=20, k_faiss=20, top_k=5, rerank=False):\n",
    "    q_tok = nltk.word_tokenize(query.lower())\n",
    "    bm25_top = bm25.get_top_n(q_tok, list(range(len(df))), n=k_bm25)\n",
    "    # vector\n",
    "    q_emb = embed_model.encode(query, convert_to_numpy=True)\n",
    "    q_emb = q_emb / np.linalg.norm(q_emb)\n",
    "    D,I = index.search(np.array([q_emb.astype('float32')]), k_faiss)\n",
    "    faiss_ids = I[0].tolist()\n",
    "    # merge\n",
    "    candidates = []\n",
    "    for i in bm25_top + faiss_ids:\n",
    "        if i not in candidates: candidates.append(i)\n",
    "    # rerank by dot product\n",
    "    cand_embs = embs[candidates]\n",
    "    sims = (cand_embs @ q_emb).reshape(-1)\n",
    "    ranked = sorted(zip(candidates,sims), key=lambda x:x[1], reverse=True)[:top_k]\n",
    "    results = [{\"idx\":int(i), \"score\":float(s), \"verse\":df.loc[int(i),'verse_number'], \"text\":df.loc[int(i),'combined_en']} for i,s in ranked]\n",
    "    # optionally cross-encoder rerank here\n",
    "    return results\n",
    "\n",
    "# test\n",
    "print(hybrid_retrieve(\"What is karma?\", top_k=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d6a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use multilingual embeddings + BERTopic\n",
    "from bertopic import BERTopic\n",
    "umap_model = umap.UMAP(n_neighbors=15, n_components=5, metric='cosine')\n",
    "cluster_model = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "topic_model = BERTopic(umap_model=umap_model, hdbscan_model=cluster_model, embedding_model=embed_model, calculate_probabilities=False)\n",
    "topics, probs = topic_model.fit_transform(df['combined_en'].tolist())\n",
    "df['topic'] = topics\n",
    "topic_model.get_topic_info().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b61c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HuggingFace transformers summarization pipeline (optional)\n",
    "from transformers import pipeline\n",
    "sum_model = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")  # or use larger\n",
    "def summarize_text(t, max_len=120):\n",
    "    try:\n",
    "        res = sum_model(t, max_length=max_len, min_length=30, do_sample=False)\n",
    "        return res[0]['summary_text']\n",
    "    except Exception as e:\n",
    "        return t[:250]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\",\"YOUR_KEY\")\n",
    "llm = OpenAI(temperature=0.2, max_tokens=300)\n",
    "hf_emb = HuggingFaceEmbeddings(model_name=EMB_MODEL_NAME)\n",
    "chroma_store = Chroma(collection_name=\"verses\", persist_directory=str(ARTIFACT_DIR/\"chroma\"), embedding=hf_emb)\n",
    "retriever = chroma_store.as_retriever(search_kwargs={\"k\":4})\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=False)\n",
    "prompt_template = \"\"\"\n",
    "You are Divyavaani AI — a careful, scripture-based assistant.\n",
    "Use only the CONTEXT and CHAT HISTORY to answer. If insufficient, say \"I don't know\".\n",
    "\n",
    "CHAT HISTORY:\n",
    "{chat_history}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"chat_history\",\"context\",\"question\"], template=prompt_template)\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory, combine_docs_chain_kwargs={\"prompt\":PROMPT})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ee79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short-term: conversation buffer via LangChain memory\n",
    "# Long-term: persist JSON + add to vector DB as user notes (learned knowledge)\n",
    "\n",
    "LONGTERM_PATH = ARTIFACT_DIR/\"longterm_memory.json\"\n",
    "if LONGTERM_PATH.exists():\n",
    "    longterm = json.load(open(LONGTERM_PATH))\n",
    "else:\n",
    "    longterm = {}\n",
    "\n",
    "def persist_memory(user, q, a):\n",
    "    longterm.setdefault(user,[]).append({\"q\":q,\"a\":a})\n",
    "    json.dump(longterm, open(LONGTERM_PATH,\"w\"), indent=2)\n",
    "    # optionally add to Chroma as user_note\n",
    "    user_doc = f\"UserNote: Q:{q} A:{a}\"\n",
    "    new_id = f\"user_{user}_{len(longterm[user])}\"\n",
    "    chroma_store.add_documents([user_doc], metadatas=[{\"source\":\"user_note\",\"user\":user}], ids=[new_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097dc4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "def to_en(text):\n",
    "    try: return translator.translate(text, dest='en').text\n",
    "    except: return text\n",
    "\n",
    "def to_hi(text):\n",
    "    try: return translator.translate(text, dest='hi').text\n",
    "    except: return text\n",
    "\n",
    "def ask(user, text):\n",
    "    # detect language\n",
    "    try: lang = detect(text)\n",
    "    except: lang = 'en'\n",
    "    if lang!='en': q_en = to_en(text)\n",
    "    else: q_en = text\n",
    "    # retrieve contexts\n",
    "    contexts = hybrid_retrieve(q_en, top_k=6)\n",
    "    ctx_text = \"\\n\\n\".join([f\"[{c['verse']}] {c['text'][:400]}\" for c in contexts])\n",
    "    # use RAG chain with memory\n",
    "    out = rag_chain({\"question\": q_en})\n",
    "    answer = out.get('answer') if isinstance(out, dict) else out\n",
    "    # optionally translate back\n",
    "    if lang=='hi':\n",
    "        answer_out = to_hi(answer)\n",
    "    else:\n",
    "        answer_out = answer\n",
    "    # learning: persist\n",
    "    persist_memory(user, text, answer_out)\n",
    "    # return sources\n",
    "    srcs = [c['verse'] for c in contexts]\n",
    "    return {\"answer\":answer_out, \"sources\":srcs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval recall@k example (requires small eval set)\n",
    "def recall_at_k(query, true_idx_list, k=5):\n",
    "    res = hybrid_retrieve(query, top_k=k)\n",
    "    retrieved = [r['idx'] for r in res]\n",
    "    hits = sum(1 for t in true_idx_list if t in retrieved)\n",
    "    return hits / max(1, len(true_idx_list))\n",
    "\n",
    "# Generation metrics (BLEU/ROUGE/Embedding similarity)\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "def gen_metrics(pred, ref):\n",
    "    b = sentence_bleu([ref.split()], pred.split())\n",
    "    r = rouge.compute(predictions=[pred], references=[ref])\n",
    "    return {\"bleu\":b, **r}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paraphrase via sentence-transformers backtranslation or small models\n",
    "from transformers import pipeline\n",
    "# example using a paraphrase model (optional heavy)\n",
    "# para = pipeline(\"text2text-generation\", model=\"Vamsi/T5_Paraphrase_Paws\")\n",
    "def paraphrase(text):\n",
    "    # lightweight: return text (placeholder) or use online APIs\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = easyocr.Reader(['en','hi'])  # supports Hindi Devanagari partially\n",
    "def ocr_image(path):\n",
    "    res = reader.readtext(path, detail=0)\n",
    "    return \" \".join(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b0c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save artifacts\n",
    "df.to_parquet(ARTIFACT_DIR/'verses.parquet', index=False)\n",
    "np.save(ARTIFACT_DIR/'embeddings.npy', embs)\n",
    "faiss.write_index(index, str(ARTIFACT_DIR/'faiss.index'))\n",
    "print(\"Saved artifacts:\", ARTIFACT_DIR)\n",
    "\n",
    "# FastAPI minimal snippet (save as api.py later)\n",
    "fastapi_snippet = '''\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "app = FastAPI()\n",
    "class QA(BaseModel):\n",
    "    user:str; question:str\n",
    "@app.post(\"/query\")\n",
    "def query(q:QA):\n",
    "    # load models & call ask(user, question)\n",
    "    return {\"answer\":\"(demo)\",\"sources\":[]}\n",
    "'''\n",
    "print(fastapi_snippet[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258c8af",
   "metadata": {},
   "source": [
    "1) Use managed vector DB for scale: Pinecone / Weaviate / Chroma Cloud.\n",
    "2) Use instruction-tuned LLMs (OpenAI GPT-4o/4o-realtime or Anthropic) with system prompts to avoid hallucination.\n",
    "3) Cache LLM responses & rate-limit; persist long-term memory in secure DB.\n",
    "4) For Sanskrit-specific tasks, create domain lexicons and manually-curated mapping for sensitive verses.\n",
    "5) Logging & human-in-loop review for high-sensitivity outputs (religious interpretation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e0d6c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
