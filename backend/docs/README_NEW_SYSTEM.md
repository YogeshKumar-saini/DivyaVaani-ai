# ğŸš€ Scalable Pipeline Architecture - Complete System

## Overview

A **world-class, production-ready document processing pipeline** for spiritual texts that supports multiple collections, intelligent caching, powerful search, and unlimited features.

## âš¡ Quick Start (5 Minutes)

```bash
# 1. Test the system
python test_pipeline.py

# 2. List available collections
python cli.py list-collections

# 3. Process Bhagavad Gita
python cli.py run --collection bhagavad_gita

# 4. Check status
python cli.py status --collection bhagavad_gita

# 5. Try the APIs
python example_usage.py
```

## ğŸ¯ What This System Does

### Before (Old System)
- âŒ Single collection only (Bhagavad Gita)
- âŒ Monolithic pipeline
- âŒ No caching
- âŒ Hard to add new formats
- âŒ Difficult to build multiple features

### After (New System)
- âœ… Multiple collections (Bhagavad Gita, Ramayana, Mahabharata, etc.)
- âœ… Modular pipeline (5 independent stages)
- âœ… Intelligent caching (avoid recomputation)
- âœ… Easy to add formats (CSV, Excel, PDF, etc.)
- âœ… Data access APIs for building features

## ğŸ“¦ System Components

### 1. Pipeline System
Process documents through 5 stages:
```
Ingestion â†’ Validation â†’ Cleaning â†’ Embedding â†’ Indexing
```

### 2. Document Processors
- **CSV Processor** - Handle CSV files with any schema
- **Excel Processor** - Handle .xlsx and .xls files
- **Extensible** - Easy to add PDF, JSON, XML, etc.

### 3. Collection Management
- Create and manage multiple collections
- Track processing status
- View statistics

### 4. Embedding Service
- Generate vector embeddings
- Intelligent caching
- Batch optimization

### 5. Data Access APIs
- **CollectionAPI** - Access documents
- **RetrievalAPI** - Search across collections
- **Hybrid Search** - Combine vector + BM25

### 6. CLI Tool
- `run` - Process collections
- `list-collections` - Show all collections
- `status` - Check processing status
- `list-stages` - Show pipeline stages

## ğŸ“ Directory Structure

```
.
â”œâ”€â”€ cli.py                      # Command-line interface
â”œâ”€â”€ test_pipeline.py            # System test
â”œâ”€â”€ example_usage.py            # API examples
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ collections.yaml        # Collection configurations
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline/              # Pipeline orchestration
â”‚   â”œâ”€â”€ embeddings/            # Embedding service + cache
â”‚   â”œâ”€â”€ storage/               # Collection management
â”‚   â”œâ”€â”€ data_access/           # Data access APIs
â”‚   â”œâ”€â”€ config/                # Configuration loaders
â”‚   â””â”€â”€ monitoring/            # Metrics and logging
â”‚
â”œâ”€â”€ artifacts/                 # Processed collections
â”‚   â””â”€â”€ {collection}/
â”‚       â”œâ”€â”€ embeddings.npy
â”‚       â”œâ”€â”€ faiss.index
â”‚       â”œâ”€â”€ bm25.pkl
â”‚       â”œâ”€â”€ chroma/
â”‚       â”œâ”€â”€ documents.parquet
â”‚       â””â”€â”€ manifest.json
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ QUICK_START.md
    â”œâ”€â”€ PIPELINE_README.md
    â”œâ”€â”€ FINAL_SUMMARY.md
    â””â”€â”€ TASK_COMPLETION_STATUS.md
```

## ğŸ¨ Pre-Configured Collections

| Collection | Files | Format | Status |
|-----------|-------|--------|--------|
| Bhagavad Gita | 1 | CSV | âœ… Enabled |
| Ramayana | 5 | CSV | âœ… Enabled |
| Mahabharata | 1 | CSV | â¸ï¸ Disabled |
| Mahapuranas | 1 | Excel | â¸ï¸ Disabled |

## ğŸ’» Usage Examples

### CLI Usage

```bash
# Process a collection
python cli.py run --collection bhagavad_gita

# List all collections
python cli.py list-collections

# Check status
python cli.py status --collection bhagavad_gita

# Run specific stages only
python cli.py run --collection ramayana --start-stage cleaning
```

### Python API Usage

#### Access Documents
```python
from src.data_access import CollectionAPI

api = CollectionAPI(artifact_dir="artifacts")

# Get documents
docs = api.get_documents("bhagavad_gita", limit=10)

# Get specific document
doc = api.get_document_by_id("bhagavad_gita", "doc_id")

# Count documents
count = api.count_documents("bhagavad_gita")
```

#### Search Across Collections
```python
from src.data_access import RetrievalAPI
from src.embeddings import EmbeddingService

# Initialize
embedding_service = EmbeddingService(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    enable_cache=True
)

api = RetrievalAPI(
    artifact_dir="artifacts",
    embedding_service=embedding_service
)

# Vector search
results = api.search(
    query="dharma and duty",
    collections=["bhagavad_gita", "ramayana"],
    top_k=5
)

# Hybrid search (vector + BM25)
results = api.hybrid_search(
    query="karma yoga",
    collections=["bhagavad_gita"],
    top_k=5
)
```

## ğŸ”§ Configuration

### Add a New Collection

Edit `config/collections.yaml`:

```yaml
collections:
  my_collection:
    source_files:
      - data/my_file.csv
    processor: csv
    schema_mapping:
      content: text_column
      metadata:
        - author
        - chapter
    embedding_model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
    enabled: true
```

Then run:
```bash
python cli.py run --collection my_collection
```

## ğŸ¯ Key Features

### âœ… Modular Architecture
- Clean separation of concerns
- Easy to extend and maintain
- Plugin architecture

### âœ… Multi-Collection Support
- Process multiple books independently
- Separate artifacts per collection
- Collection-level configuration

### âœ… Intelligent Caching
- Cache embeddings to avoid recomputation
- Significant performance improvement
- Cache statistics tracking

### âœ… Powerful Search
- Vector similarity (FAISS)
- Text search (BM25)
- Hybrid search (combines both)
- Cross-collection search

### âœ… Production Ready
- Comprehensive error handling
- Metrics and monitoring
- Resumable execution
- Detailed logging

## ğŸ“Š Output Artifacts

After processing, each collection has:

```
artifacts/{collection}/
â”œâ”€â”€ embeddings.npy              # Vector embeddings
â”œâ”€â”€ faiss.index                 # FAISS vector index
â”œâ”€â”€ bm25.pkl                    # BM25 text index
â”œâ”€â”€ chroma/                     # ChromaDB collection
â”œâ”€â”€ documents.parquet           # Processed documents
â”œâ”€â”€ manifest.json               # Processing metadata
â”œâ”€â”€ metrics.json                # Performance metrics
â””â”€â”€ collection_manifest.json    # Collection info
```

## ğŸš€ Building Features

Use the data access APIs to build:

### QA System
```python
from src.data_access import RetrievalAPI

# Search for relevant context
results = api.search(query, collections, top_k=5)

# Pass to LLM for answer generation
context = "\n".join([r.content for r in results])
answer = llm.generate(query, context)
```

### Search Engine
```python
# Semantic search
results = api.search(query, collections, top_k=10)

# Display results with scores
for result in results:
    print(f"{result.rank}. {result.content[:100]}...")
    print(f"   Score: {result.score:.4f}")
```

### Analytics Dashboard
```python
from src.storage import CollectionManager

manager = CollectionManager("artifacts")

# Get stats for all collections
for coll in manager.list_collections():
    stats = manager.get_collection_stats(coll.name)
    print(f"{coll.name}: {stats.document_count} docs")
```

## ğŸ“š Documentation

- **QUICK_START.md** - Get started in 5 minutes
- **PIPELINE_README.md** - Complete user guide
- **FINAL_SUMMARY.md** - Comprehensive overview
- **TASK_COMPLETION_STATUS.md** - Implementation status
- **example_usage.py** - Code examples

## ğŸ” Troubleshooting

### Collection Not Found
```bash
# Check if collection is configured
python cli.py list-collections

# Verify config file
cat config/collections.yaml
```

### Import Errors
```bash
# Install dependencies
pip install pandas numpy pyyaml click sentence-transformers faiss-cpu chromadb rank-bm25 openpyxl
```

### Processing Errors
```bash
# Check logs in console output
# Review manifest for details
cat artifacts/{collection}/manifest.json
```

## ğŸ‰ Success Metrics

âœ… **Modular Pipeline** - 5 independent stages
âœ… **Multi-Format Support** - CSV + Excel (extensible)
âœ… **Multi-Collection** - Process multiple books
âœ… **Intelligent Caching** - Avoid recomputation
âœ… **Powerful Search** - Vector + BM25 + Hybrid
âœ… **Data Access APIs** - Clean interfaces
âœ… **Production Ready** - Error handling, monitoring
âœ… **Well Documented** - Complete guides
âœ… **CLI Tool** - User-friendly interface
âœ… **Extensible** - Easy to add features

## ğŸ†˜ Support

1. Check documentation files
2. Run `python test_pipeline.py`
3. Run `python example_usage.py`
4. Review logs and manifests
5. Check `PIPELINE_README.md` for details

## ğŸ“ License

[Your License Here]

## ğŸ™ Acknowledgments

Built with:
- Python 3.8+
- pandas, numpy
- sentence-transformers
- FAISS, ChromaDB, BM25
- Click, PyYAML

---

## ğŸ¤ LiveKit Voice Agent Integration

### Overview

The system now includes **real-time voice AI capabilities** powered by LiveKit, enabling users to have natural voice conversations with the Bhagavad Gita wisdom through advanced speech-to-speech interaction.

### Architecture

```
User Speech â†’ LiveKit STT (Whisper) â†’ RAG Query â†’ LLM Response â†’ Cartesia TTS â†’ User Audio
```

### Features

- **Real-time Voice Interaction** - Speak naturally and receive immediate audio responses
- **Multilingual Support** - English, Hindi, Sanskrit voice processing
- **Spiritual Context** - Responses framed as divine wisdom from Krishna
- **Integrated RAG** - Direct access to your existing knowledge base
- **Production Ready** - Scalable, monitored, and enterprise-grade

### Setup

1. **Install Dependencies**
```bash
pip install -r requirements-api.txt
```

2. **Configure Environment**
```bash
# Copy and edit .env
cp .env.example .env

# Add your API keys
LIVEKIT_URL=wss://your-project.livekit.cloud
LIVEKIT_API_KEY=your_livekit_api_key
LIVEKIT_API_SECRET=your_livekit_api_secret
CARTESIA_API_KEY=your_cartesia_api_key
```

3. **Run Voice Agent**
```bash
python src/rag/voice_agent/livekit_agent.py start
```

### Voice Agent Components

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Speech-to-Text** | OpenAI Whisper | Convert user speech to text |
| **Voice Activity Detection** | Silero VAD | Detect when user is speaking |
| **Language Model** | GPT-4o | Generate contextual responses |
| **Text-to-Speech** | Cartesia Sonic | Convert responses to natural speech |
| **Real-time Transport** | LiveKit | Handle audio streaming and rooms |

### Integration Points

- **RAG Backend** - Uses your existing `MultilingualQASystem`
- **Knowledge Base** - Access to all processed collections
- **Memory Management** - Conversation context across turns
- **Analytics** - Track voice query metrics
- **Caching** - Intelligent response caching

### Usage Examples

#### Start Voice Session
```python
from src.rag.voice_agent.livekit_agent import initialize_system

# Initialize the voice-enabled QA system
qa_system = await initialize_system()

# The agent will connect to LiveKit room and start listening
```

#### Voice Query Processing
```python
# The agent automatically handles:
# 1. Speech recognition
# 2. Query understanding
# 3. RAG retrieval
# 4. Response generation
# 5. Text-to-speech synthesis
```

### Configuration Options

```python
# Voice settings in src/config/voice_config.py
LIVEKIT_URL=your_livekit_server
LIVEKIT_API_KEY=your_api_key
LIVEKIT_API_SECRET=your_secret
CARTESIA_API_KEY=your_tts_key
STT_MODEL=whisper-1
TTS_VOICE=bf0a246a-8642-498a-9950-80c35e9276b5
```

### Future Enhancements

- **Multilingual Voices** - Hindi and Sanskrit TTS support
- **Conversation Memory** - Maintain context across sessions
- **Session Analytics** - Voice interaction metrics
- **Frontend Integration** - Web SDK for browser-based voice chat
- **Multi-user Rooms** - Group voice discussions

### API Endpoints

The voice agent integrates with existing FastAPI endpoints:

- `POST /voice/` - Voice query processing
- `POST /voice/stt/` - Speech-to-text conversion
- `POST /voice/tts/` - Text-to-speech synthesis

### Monitoring & Analytics

Voice interactions are tracked through the existing analytics system:

```python
# View voice analytics
analytics = qa_system.get_analytics()
print(analytics["voice_interactions"])
```

---

**Status**: âœ… **PRODUCTION READY**

Start processing your collections now:
```bash
python cli.py run --collection bhagavad_gita
```

ğŸ¤ **Voice-enabled spiritual guidance ready!**

ğŸš€ **Happy building!**
